This game is for playing out situations like the outbreak of a new disease, an alien invasion, some dangerous (or promising) new global phenomenon, etc. The scale of the scenarios should be such that its less about individually going and resolving things by hand, and more about creating the tools for millions of other people to deploy to resolve the tension. Not all research in this game has to be on a timer, but a lot of things won't matter as much unless there is *some* overarching urgency on the scale of months or a few years, which can be resolved through research and subsequent engineering or development.

Research is organized into Projects, which each center around a graph of linked phenomena - this is a set of things which share a common root cause, but which can themselves have downstream effects and so on.

For example, perhaps the initiating event is that people start to die off in the highest altitude villages on Earth. That would be a single Phenomenon, a node in the causal graph. Upstream of that node could be, for example, the Phenomenon that odd genetic changes have happened across humanity within a short period of time. Downstream of the genetic changes could be other localized consequences of those changes (each its own Phenomenon), and upstream might be e.g. 'an alien probability manipulation engine in Antarctica has turned on, and is adjusting microscopic random events to bring about a specific future event' (which would have other downstream nodes as well, like odd winning streaks in casinos in South America, etc.

The Project starts from a particular node on the graph, but characters don't know if that's a leaf node or a root node or what. By gathering evidence about a Phenomenon, characters can uncover linked nodes, with the goal of (ostensibly) uncovering a node they can actually do something about. Evidence takes the form of Data, Samples, and Experiments, each of which require interacting with the Phenomenon in distinct ways. Different sources of Evidence have Quality levels (1, 2, or 3) and keywords that describe in more detail the type of evidence it is (Genetic, Behavioral, Material, etc) and which determine which skills and equipment apply to processing that evidence.

- Samples are generally fast but personally risky. People need to go into the field, encounter the Phenomenon directly, etc. A Low Quality (Lv1) sample could be collecting blood from random people living at altitude; a Medium Quality (Lv2) sample could be finding someone who is in the process of dying of the ailment and being able to monitor their progress directly. A High Quality (Lv3) sample might be finding someone who appears to be immune when the rest of their family were infected. The more specific the Sample is to the core of the Phenomenon, the higher the Quality. Analysis equipment can increase the Quality of Samples bearing specific keywords. Samples are most useful for understanding a particular node of the causal graph itself (which is a precondition for doing Experiments and ultimately devising Manipulations). Mechanically, the way this works is that each Phenomenon has a set of traits or keywords associated with different minimum sample qualities, and a roll modified by the head researcher's appropriate skill, the sample quality, and any lab context determines whether the highest viable trait is revealed, a lesser trait is revealed, or no new information is gained.

- Data involves large-scale aggregation of information about the Phenomenon - this is low risk but takes time and is dependent on Infrastructure (which can be constructed but itself takes time and money) - time and Infrastructure basically determine the quality of Data collected. Data is best suited for unlocking adjacent nodes of the causal graph. The way this works is similar to with Samples, but here the upstream node requires the highest quality to unlock; however, every parallel node which shares that upstream node that the researchers are aware of reduces that quality threshold by 1.

- Experiments require enough understanding of a particular node to form a hypothesis (corresponding to knowing a trait or keyword of the given node), but if successful they unlock the ability to manipulate or even replicate the behavior of a Phenomenon node. Ultimately, something like a plague would be resolved by finding a good enough bottleneck node (such that all the lethal consequences are downstream of that node) which one can also understand well enough to complete an Experiment, and then use that to devise a cure. The tricky thing is that the specific ways in which a given Phenomenon can be manipulated are based on the keywords associated with that Phenomenon - finding out that the high altitude deaths are due to genetic changes isn't helpful if you don't have the ability to change everyone's genome, for example. Over campaign-length timescales, General Research can be accumulated which can allow for new kinds of node manipulation.

General Research and Technologies

The current set of Technologies are those things which one can do given understanding of a Phenomenon with particular keywords. For example, a Technology could be 'Prevent the effects of a Genetic Node' or 'Recreate the effects of a Particle node'. There is a base set of technologies that the civilization the researchers are in starts with. As Projects are completed and nodes are understood and manipulated, this generates XP which can be spent to unlock new forms of manipulation.
